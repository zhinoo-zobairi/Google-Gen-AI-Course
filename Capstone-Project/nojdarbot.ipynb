{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11387191,"sourceType":"datasetVersion","datasetId":7130639},{"sourceId":11388976,"sourceType":"datasetVersion","datasetId":7132017}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NojdarBot: Multilingual Medical Question Answering with Generative AI\n\nWelcome to this Kaggle notebook where we build a complete medical question answering system for Kurdish speakers using generative AI. By the end of this notebook, you'll have created a working system that translates Kurdish medical questions, retrieves relevant information, and provides accurate answers in the user's native language.\n\n## 1. The Problem: Medical Information Access for Kurdish Speakers\n\nAccess to accurate medical information in languages other than English presents a significant global healthcare challenge. For the ~30 million Kurdish speakers worldwide, particularly those who speak Sorani Kurdish, finding reliable medical information in their native language is extremely difficult.\n\nTraditional translation services often fail with medical content due to:\n- Lack of specialized medical vocabulary\n- Cultural context misalignment\n- Inaccurate translations that could lead to dangerous medical misunderstandings\n\n## 2. How Generative AI Solves This Problem\n\nNojdarBot leverages multiple AI models in a custom pipeline:\n\n1. **Translation**: Gemini 1.5 Pro translates Kurdish questions to English\n2. **Retrieval**: ChromaDB vector database retrieves medical knowledge\n3. **Answer Generation**: Gemini generates medically accurate responses\n4. **Evaluation**: Claude 3.5 Haiku assesses response quality\n5. **Back-Translation**: Specialized translation preserves medical terminology\n\nThe following sections walk through building this system step by step.\n\"\"\"\n\n\"\"\"\n## 3. Environment Setup\n\nFirst, we install the necessary libraries and import dependencies. We'll need:\n- `chromadb`: For vector storage of medical knowledge\n- `google-generativeai`: To access Gemini models for translation and answer generation\n- `anthropic`: To access Claude models for quality evaluation\n- `langgraph`: For building our processing pipeline\n\nWe also import standard utilities for working with data, regex, and JSON\n\n### 3.1 Environment Setup","metadata":{}},{"cell_type":"code","source":"!pip install -q chromadb google-generativeai anthropic langgraph\nimport ast\nimport os\nimport re\nimport numpy as np\nimport chromadb\nfrom google import genai\nfrom google.genai import types\nfrom google.api_core import retry\nimport json\nfrom datetime import datetime","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T06:45:15.074542Z","iopub.execute_input":"2025-04-21T06:45:15.075007Z","iopub.status.idle":"2025-04-21T06:45:19.827970Z","shell.execute_reply.started":"2025-04-21T06:45:15.074955Z","shell.execute_reply":"2025-04-21T06:45:19.826751Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"Next, we define an enumeration to represent the quality rating levels for our evaluation system.\nThis gives us a consistent way to categorize and reference the quality of generated answers.","metadata":{}},{"cell_type":"markdown","source":"### 3.2 Enumerations for Quality Ratings\n","metadata":{}},{"cell_type":"code","source":"import enum\n\nclass AnswerRating(enum.Enum):\n    VERY_GOOD = '5'\n    GOOD = '4'\n    OK = '3'\n    BAD = '2'\n    VERY_BAD = '1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T06:45:19.829669Z","iopub.execute_input":"2025-04-21T06:45:19.829935Z","iopub.status.idle":"2025-04-21T06:45:19.835334Z","shell.execute_reply.started":"2025-04-21T06:45:19.829911Z","shell.execute_reply":"2025-04-21T06:45:19.834205Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## 4. API Configuration and Model Access\n\nIn this section, we set up access to the AI models we'll use:\n\n1. **Gemini API**: For translation and answer generation\n2. **Claude API**: For quality evaluation of generated answers\n\nWe use Kaggle secrets to securely access API keys, which protects them from being exposed in the notebook.","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nfrom anthropic import Anthropic\n\n\nuser_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"GOOGLE_API_KEY\")\nclient = genai.Client(api_key=api_key)\n\n\nclaude_api_key = user_secrets.get_secret(\"ANTHROPIC_API_KEY\")\nclaude_client = Anthropic(api_key=claude_api_key)","metadata":{"execution":{"iopub.status.busy":"2025-04-21T06:45:19.837249Z","iopub.execute_input":"2025-04-21T06:45:19.837527Z","iopub.status.idle":"2025-04-21T06:45:20.185135Z","shell.execute_reply.started":"2025-04-21T06:45:19.837502Z","shell.execute_reply":"2025-04-21T06:45:20.184028Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## 5. Translation Pipeline: The Intake Node\n\nThis first node in our pipeline handles the translation of Sorani Kurdish medical questions to English.\n\nThe function:\n1. Receives user input in Kurdish\n2. Uses Gemini 1.5 Pro with a specialized prompt to translate the question\n3. Stores both the input and translation in the state object\n4. Maintains conversation history for context-aware translations\n\nThis translation step is critical - the quality of all subsequent steps depends on accurate translation of the medical terminology.\n","metadata":{}},{"cell_type":"code","source":"def intake_node(state: dict) -> dict:\n    # === Step 1: Prepare conversation history prompt ===\n    history_prompt = \"\\n\".join(\n        [f\"User: {turn['user']}\\nAgent: {turn['agent']}\" for turn in state.get(\"chat_history\", [])]\n    )\n    \n    # === Step 2: Construct translation prompt with history context ===\n    prompt = (\n        f\"{history_prompt}\\n\"\n        f\"User: Translate this Sorani Kurdish medical question to English:\\n{state['user_input']}\\n\"\n        f\"Agent:\"\n    )\n\n    # === Step 3: Configure and call the translation model ===\n    config = types.GenerateContentConfig(temperature=0.3)\n    response = client.models.generate_content(\n        model=\"gemini-1.5-pro-latest\",\n        config=config,\n        contents=[prompt]\n    )\n\n    # === Step 4: Process result and update state ===\n    translated = response.text.strip()\n    state[\"translated_input\"] = translated\n\n    state[\"chat_history\"] = state.get(\"chat_history\", [])\n    state[\"chat_history\"].append({\n        \"user\": state[\"user_input\"],\n        \"agent\": translated\n    })\n\n    return state","metadata":{"execution":{"iopub.status.busy":"2025-04-21T06:45:20.186645Z","iopub.execute_input":"2025-04-21T06:45:20.186927Z","iopub.status.idle":"2025-04-21T06:45:20.193626Z","shell.execute_reply.started":"2025-04-21T06:45:20.186902Z","shell.execute_reply":"2025-04-21T06:45:20.192386Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## 6. Knowledge Base: Building the Medical Information Retrieval System\n\nIn the next sections, we build our Retrieval-Augmented Generation (RAG) system, which provides the knowledge base for our medical questions.\n\nThe steps include:\n1. Parsing a medical Q&A dataset (MedQuAD) into structured data\n2. Creating vector embeddings of these Q&A pairs\n3. Storing the embeddings in ChromaDB for semantic search\n4. Setting up efficient retrieval functionality\n\nThis approach ensures our answers are grounded in reliable medical information rather than being hallucinated.","metadata":{}},{"cell_type":"markdown","source":"# 6.1 Dataset Parser for MedQuAD","metadata":{}},{"cell_type":"code","source":"import xml.etree.ElementTree as ET\n\ndef parse_medquad_folder(base_path):\n    qa_pairs = []\n    for root_dir, _, files in os.walk(base_path):\n        for fname in files:\n            if fname.endswith(\".xml\"):\n                fpath = os.path.join(root_dir, fname)\n                try:\n                    tree = ET.parse(fpath)\n                    root = tree.getroot()\n                    for qa in root.findall(\".//QAPair\"):\n                        question_elem = qa.find(\"Question\")\n                        answer_elem = qa.find(\"Answer\")\n                        if question_elem is not None and answer_elem is not None:\n                            question = question_elem.text.strip() if question_elem.text else None\n                            answer = answer_elem.text.strip() if answer_elem.text else None\n                            if question and answer:\n                                qa_pairs.append({\n                                    \"question\": question,\n                                    \"answer\": answer,\n                                    \"source_file\": fname,\n                                    \"source_folder\": os.path.basename(root_dir),\n                                    \"pid\": qa.attrib.get(\"pid\", \"N/A\"),\n                                    \"qid\": question_elem.attrib.get(\"qid\", \"N/A\"),\n                                    \"qtype\": question_elem.attrib.get(\"qtype\", \"N/A\")\n                                })\n                except Exception as e:\n                    print(f\"‚ùå Error parsing {fname}: {e}\")\n    return qa_pairs","metadata":{"execution":{"iopub.status.busy":"2025-04-21T06:45:20.194763Z","iopub.execute_input":"2025-04-21T06:45:20.195118Z","iopub.status.idle":"2025-04-21T06:45:20.210669Z","shell.execute_reply.started":"2025-04-21T06:45:20.195091Z","shell.execute_reply":"2025-04-21T06:45:20.209632Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"### 6.2 Vector Database Setup with ChromaDB\n\nNow we initialize our vector database to store and retrieve medical knowledge efficiently. ChromaDB provides:\n- Persistent storage of document embeddings\n- Fast similarity search using cosine distance\n- Metadata storage for source tracking\n\nThe system checks if the database already exists. If not, it processes the MedQuAD dataset, \nextracts question-answer pairs, generates embeddings using Gemini's embedding model, and stores them in ChromaDB.","metadata":{}},{"cell_type":"code","source":"# === Step 1: Folder path for MedQuAD on Kaggle ===\nfolder = \"/kaggle/input/medquad-master-zip/MedQuAD-master\"\n\n# === Step 2: Initialize Chroma Persistent Client ===\nchroma_client = chromadb.PersistentClient(path=\"./chroma_storage\")\ncollection = chroma_client.get_or_create_collection(\n    name=\"medical-knowledge\",\n    metadata={\n        \"hnsw:space\": \"cosine\",\n        \"hnsw:search_ef\": 100,\n        \"hnsw:construction_ef\": 100,\n    }\n)\n\n# === Step 3: Check if collection is already populated ===\nif collection.count() == 0:\n    print(\"No documents found in Chroma. Ingesting and embedding now...\")\n\n    # === Step 4: Define folders to parse ===\n    folders_to_parse = [\n        \"1_CancerGov_QA\", \"2_GARD_QA\", \"3_GHR_QA\", \"4_MPlus_Health_Topics_QA\",\n        \"5_NIDDK_QA\", \"6_NINDS_QA\", \"7_SeniorHealth_QA\", \"9_CDC_QA\"\n    ]\n\n    retrievable_docs = []\n    for name in folders_to_parse:\n        folder_path = os.path.join(folder, name)\n        docs = parse_medquad_folder(folder_path)\n        print(f\"Extracted {len(docs)} question‚Äìanswer pairs from {folder_path}\")\n        retrievable_docs.extend(docs)\n\n    # === Step 5: Build corpus and metadata ===\n    corpus = [f\"{item['question']}\\n{item['answer']}\" for item in retrievable_docs]\n    metadatas = [\n        {\n            \"qid\": item[\"qid\"],\n            \"qtype\": item[\"qtype\"],\n            \"pid\": item[\"pid\"],\n            \"source_file\": item[\"source_file\"],\n            \"source_folder\": item[\"source_folder\"]\n        }\n        for item in retrievable_docs\n    ]\n\n    # === Step 6: Define Gemini Embedding Function with batching ===\n    is_retriable = lambda e: isinstance(e, genai.errors.APIError) and e.code in {429, 503}\n\n    @retry.Retry(predicate=is_retriable, timeout=300.0)\n    def safe_embed_text_batches(corpus, task_type=\"retrieval_document\", batch_size=100):\n        all_embeddings = []\n        for start in range(0, len(corpus), batch_size):\n            end = min(start + batch_size, len(corpus))\n            batch = corpus[start:end]\n            print(f\"Embedding batch {start} to {end}...\")\n            try:\n                response = client.models.embed_content(\n                    model=\"models/text-embedding-004\",\n                    contents=batch,\n                    config=types.EmbedContentConfig(task_type=task_type)\n                )\n                batch_embeddings = [e.values for e in response.embeddings]\n                all_embeddings.extend(batch_embeddings)\n            except Exception as e:\n                print(f\"Error embedding batch {start}-{end}: {e}\")\n        return all_embeddings\n\n    print(\"Generating document embeddings using Gemini...\")\n    doc_embeddings = safe_embed_text_batches(corpus)\n\n    # === Step 7: Upload to Chroma in batches ===\n    def batch_add_to_chroma(collection, corpus, embeddings, metadatas, batch_size=5000):\n        total = len(corpus)\n        for start in range(0, total, batch_size):\n            end = min(start + batch_size, total)\n            print(f\"Adding Chroma batch {start} to {end}...\")\n            collection.add(\n                documents=corpus[start:end],\n                embeddings=embeddings[start:end],\n                metadatas=metadatas[start:end],\n                ids=[f\"doc_{i}\" for i in range(start, end)]\n            )\n\n    batch_add_to_chroma(\n        collection=collection,\n        corpus=corpus,\n        embeddings=doc_embeddings,\n        metadatas=metadatas,\n        batch_size=5000\n    )\n\n    # === Step 8: Done\n    print(f\"Successfully indexed {len(corpus)} documents into ChromaDB.\")\n\nelse:\n    print(f\"ChromaDB already contains {collection.count()} documents. Skipping ingestion.\")","metadata":{"execution":{"iopub.status.busy":"2025-04-21T06:45:20.211799Z","iopub.execute_input":"2025-04-21T06:45:20.212101Z","iopub.status.idle":"2025-04-21T06:45:20.238395Z","shell.execute_reply.started":"2025-04-21T06:45:20.212077Z","shell.execute_reply":"2025-04-21T06:45:20.237294Z"},"trusted":true},"outputs":[{"name":"stdout","text":"ChromaDB already contains 15848 documents. Skipping ingestion.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"print(dir(collection))","metadata":{"execution":{"iopub.status.busy":"2025-04-21T06:45:20.239525Z","iopub.execute_input":"2025-04-21T06:45:20.239902Z","iopub.status.idle":"2025-04-21T06:45:20.258867Z","shell.execute_reply.started":"2025-04-21T06:45:20.239867Z","shell.execute_reply":"2025-04-21T06:45:20.257900Z"},"trusted":true},"outputs":[{"name":"stdout","text":"['__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_client', '_data_loader', '_embed', '_embed_record_set', '_embedding_function', '_is_protocol', '_model', '_transform_get_response', '_transform_peek_response', '_transform_query_response', '_update_model_after_modify_success', '_validate_and_prepare_add_request', '_validate_and_prepare_delete_request', '_validate_and_prepare_get_request', '_validate_and_prepare_query_request', '_validate_and_prepare_update_request', '_validate_and_prepare_upsert_request', '_validate_modify_request', 'add', 'configuration', 'configuration_json', 'count', 'database', 'delete', 'get', 'get_model', 'id', 'metadata', 'modify', 'name', 'peek', 'query', 'tenant', 'update', 'upsert']\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"print(f\"Collection type: {type(collection).__name__}\")","metadata":{"execution":{"iopub.status.busy":"2025-04-21T06:45:20.259878Z","iopub.execute_input":"2025-04-21T06:45:20.260276Z","iopub.status.idle":"2025-04-21T06:45:20.276617Z","shell.execute_reply.started":"2025-04-21T06:45:20.260207Z","shell.execute_reply":"2025-04-21T06:45:20.275667Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collection type: Collection\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"if hasattr(collection, \"_client\"):\n    print(f\"Client type: {type(collection._client).__name__}\")\n    \n    if hasattr(collection._client, \"get_settings\"):\n        settings = collection._client.get_settings()\n        print(f\"Settings: {settings}\")","metadata":{"execution":{"iopub.status.busy":"2025-04-21T06:45:20.280383Z","iopub.execute_input":"2025-04-21T06:45:20.280704Z","iopub.status.idle":"2025-04-21T06:45:20.294406Z","shell.execute_reply.started":"2025-04-21T06:45:20.280679Z","shell.execute_reply":"2025-04-21T06:45:20.293349Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Client type: RustBindingsAPI\nSettings: environment='' chroma_api_impl='chromadb.api.rust.RustBindingsAPI' chroma_server_nofile=None chroma_server_thread_pool_size=40 tenant_id='default' topic_namespace='default' chroma_server_host=None chroma_server_headers=None chroma_server_http_port=None chroma_server_ssl_enabled=False chroma_server_ssl_verify=None chroma_server_api_default_path=<APIVersion.V2: '/api/v2'> chroma_server_cors_allow_origins=[] is_persistent=True persist_directory='./chroma_storage' chroma_memory_limit_bytes=0 chroma_segment_cache_policy=None allow_reset=False chroma_auth_token_transport_header=None chroma_client_auth_provider=None chroma_client_auth_credentials=None chroma_server_auth_ignore_paths={'/api/v2': ['GET'], '/api/v2/heartbeat': ['GET'], '/api/v2/version': ['GET'], '/api/v1': ['GET'], '/api/v1/heartbeat': ['GET'], '/api/v1/version': ['GET']} chroma_overwrite_singleton_tenant_database_access_from_auth=False chroma_server_authn_provider=None chroma_server_authn_credentials=None chroma_server_authn_credentials_file=None chroma_server_authz_provider=None chroma_server_authz_config=None chroma_server_authz_config_file=None chroma_product_telemetry_impl='chromadb.telemetry.product.posthog.Posthog' chroma_telemetry_impl='chromadb.telemetry.product.posthog.Posthog' anonymized_telemetry=True chroma_otel_collection_endpoint='' chroma_otel_service_name='chromadb' chroma_otel_collection_headers={} chroma_otel_granularity=None migrations='apply' migrations_hash_algorithm='md5' chroma_segment_directory_impl='chromadb.segment.impl.distributed.segment_directory.RendezvousHashSegmentDirectory' chroma_segment_directory_routing_mode=<RoutingMode.ID: 'id'> chroma_memberlist_provider_impl='chromadb.segment.impl.distributed.segment_directory.CustomResourceMemberlistProvider' worker_memberlist_name='query-service-memberlist' chroma_server_grpc_port=None chroma_sysdb_impl='chromadb.db.impl.sqlite.SqliteDB' chroma_producer_impl='chromadb.db.impl.sqlite.SqliteDB' chroma_consumer_impl='chromadb.db.impl.sqlite.SqliteDB' chroma_segment_manager_impl='chromadb.segment.impl.manager.local.LocalSegmentManager' chroma_executor_impl='chromadb.execution.executor.local.LocalExecutor' chroma_query_replication_factor=2 chroma_quota_provider_impl=None chroma_rate_limiting_provider_impl=None chroma_quota_enforcer_impl='chromadb.quota.simple_quota_enforcer.SimpleQuotaEnforcer' chroma_rate_limit_enforcer_impl='chromadb.rate_limit.simple_rate_limit.SimpleRateLimitEnforcer' chroma_async_rate_limit_enforcer_impl='chromadb.rate_limit.simple_rate_limit.SimpleAsyncRateLimitEnforcer' chroma_logservice_request_timeout_seconds=3 chroma_sysdb_request_timeout_seconds=3 chroma_query_request_timeout_seconds=60 chroma_db_impl=None chroma_collection_assignment_policy_impl='chromadb.ingest.impl.simple_policy.SimpleAssignmentPolicy' chroma_coordinator_host='localhost' chroma_logservice_host='localhost' chroma_logservice_port=50052\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## 7. Query Processing: Retrieval and Answer Generation\n\nWith our knowledge base in place, we now implement the core RAG functionality:\n\n1. **Query Embedding**: Convert the translated question into a vector representation\n2. **Semantic Search**: Find the most relevant medical information in our database\n3. **Answer Generation**: Create a comprehensive answer based on the retrieved information\n\nThis multi-step process ensures that our responses are both relevant and factually accurate.","metadata":{}},{"cell_type":"markdown","source":"# 7.1 Query Embedding Function\n","metadata":{}},{"cell_type":"code","source":"@retry.Retry(predicate=lambda e: isinstance(e, genai.errors.APIError) and e.code in {429, 503}, timeout=300.0)\ndef embed_text(text, task_type=\"retrieval_query\"):\n    response = client.models.embed_content(\n        model=\"models/text-embedding-004\",\n        contents=[text],\n        config=types.EmbedContentConfig(task_type=task_type)\n    )\n    return response.embeddings[0].values","metadata":{"execution":{"iopub.status.busy":"2025-04-21T06:45:20.296552Z","iopub.execute_input":"2025-04-21T06:45:20.296846Z","iopub.status.idle":"2025-04-21T06:45:20.310043Z","shell.execute_reply.started":"2025-04-21T06:45:20.296820Z","shell.execute_reply":"2025-04-21T06:45:20.309150Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"### 7.2 Retrieval Node: Finding Relevant Medical Information\n\nThe retriever node is responsible for:\n1. Converting the English question into a vector embedding\n2. Searching the medical knowledge base for similar content\n3. Filtering results by a similarity threshold\n4. Structuring the results for the answer generation phase\n\nThis semantic search approach helps find conceptually relevant information even when exact keyword matches aren't present.\n\"\"\"","metadata":{}},{"cell_type":"code","source":"def retriever_node(state: dict) -> dict:\n    query_en = state[\"translated_input\"]\n\n    # === Step 1: Embed query\n    query_embedding = embed_text(query_en)\n\n    # === Step 2: Chroma query with distances\n    results = collection.query(\n        query_embeddings=[query_embedding],\n        n_results=5,\n        include=[\"documents\", \"metadatas\", \"distances\"]\n    )\n\n    # === Step 3: Distance-based filtering\n    threshold = 0.4\n    filtered_chunks = []\n    for doc, meta, dist in zip(results[\"documents\"][0], results[\"metadatas\"][0], results[\"distances\"][0]):\n        if dist < threshold:\n            q_a = doc.split(\"\\n\", 1)\n            question = q_a[0] if len(q_a) > 0 else \"\"\n            answer = q_a[1] if len(q_a) > 1 else \"\"\n            filtered_chunks.append({\"question\": question, \"answer\": answer, **meta})\n\n    # === Step 4: Update state\n    state[\"retrieved_chunks\"] = filtered_chunks\n    state[\"retrieved_metadata\"] = results[\"metadatas\"][0]\n    return state","metadata":{"execution":{"iopub.status.busy":"2025-04-21T06:45:20.311091Z","iopub.execute_input":"2025-04-21T06:45:20.311384Z","iopub.status.idle":"2025-04-21T06:45:20.329722Z","shell.execute_reply.started":"2025-04-21T06:45:20.311359Z","shell.execute_reply":"2025-04-21T06:45:20.328783Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"### 7.3 Answer Generation: The Recommender Node\n\nThe recommender node transforms retrieved medical information into a coherent answer:\n\n1. It formats the retrieved context into a structured prompt\n2. Provides clear instructions to the Gemini model on how to use the context\n3. Generates a factual, comprehensive answer grounded in the retrieved information\n4. Maintains conversation history for follow-up questions\n\nThe prompt engineering here is critical for ensuring the model only uses the provided information and doesn't hallucinate medical facts.\n\"\"\"","metadata":{}},{"cell_type":"code","source":"def recommender_node(state: dict) -> dict:\n    \"\"\"\n    Constructs a final prompt from retrieved chunks and generates an answer.\n    \"\"\"\n    question_en = state[\"translated_input\"]\n    context_list = state[\"retrieved_chunks\"]\n\n    # Format context into a clean list of Q:A pairs\n    context_str = \"\\n\".join(\n        f\"Q: {r['question']}\\nA: {r['answer']}\" for r in context_list\n    )\n\n    # Build the final prompt for Gemini\n    prompt = f\"\"\"You are a medical assistant.\nUse the information below to answer the patient's question accurately and clearly.\n\nContext:\n{context_str}\n\nQuestion:\n{question_en}\n\nBe factual. If unsure, say you don‚Äôt know.\nDon't summarize. Keep the same level of detail.\"\"\"\n\n    config = types.GenerateContentConfig(temperature=0.3)\n\n    response = client.models.generate_content(\n        model=\"gemini-1.5-pro-latest\",\n        config=config,\n        contents=[prompt]\n    )\n\n    state[\"ai_response_en\"] = response.text.strip()\n    state[\"chat_history\"] = state.get(\"chat_history\", [])\n    state[\"chat_history\"].append({\n        \"user\": state[\"user_input\"],\n        \"agent\": state[\"ai_response_en\"]\n    })\n    print(\"\\nüß† Chat History So Far:\")\n    for i, turn in enumerate(state[\"chat_history\"], 1):\n        print(f\"\\nTurn {i}:\")\n        print(f\"User: {turn['user']}\")\n        print(f\"Agent: {turn['agent']}\")\n    return state","metadata":{"execution":{"iopub.status.busy":"2025-04-21T06:45:20.330779Z","iopub.execute_input":"2025-04-21T06:45:20.331107Z","iopub.status.idle":"2025-04-21T06:45:20.344019Z","shell.execute_reply.started":"2025-04-21T06:45:20.331082Z","shell.execute_reply":"2025-04-21T06:45:20.343069Z"},"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"## 8. Quality Assurance: Evaluating and Refining Responses\n\nTo ensure our medical answers meet high standards, we implement an automated evaluation system using Claude 3.5 Haiku.\n\nThe evaluation assesses four key dimensions:\n1. **Instruction following**: Does the answer address the specific question?\n2. **Groundedness**: Is the answer based only on the provided medical context?\n3. **Completeness**: Does it fully address all aspects of the question?\n4. **Fluency**: Is the answer clear and easy to understand?\n\nThis evaluation provides a quality score and detailed feedback that helps improve the system.\n\n# 8.1 Evaluation Prompt Template","metadata":{}},{"cell_type":"code","source":"QA_PROMPT = \"\"\"# Instruction\nYou are an expert evaluator. Your task is to evaluate the quality of AI-generated answers to medical questions.\n\n# Evaluation\n## Metric Definition\nYou will assess instruction following, groundedness, completeness, and fluency.\n\n## Criteria\nInstruction following: Does it answer the question asked?\nGroundedness: Is it based only on the provided context?\nCompleteness: Does it fully answer?\nFluency: Is it easy to read?\n\n## Rating Rubric\n5: Very good  \n4: Good  \n3: Okay  \n2: Bad  \n1: Very bad\n\n# User Input\n{prompt}\n\n# AI-generated Response\n{response}\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2025-04-21T06:45:20.345069Z","iopub.execute_input":"2025-04-21T06:45:20.345415Z","iopub.status.idle":"2025-04-21T06:45:20.359825Z","shell.execute_reply.started":"2025-04-21T06:45:20.345375Z","shell.execute_reply":"2025-04-21T06:45:20.358983Z"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"# 8.2 Evaluator Node Implementation\n","metadata":{}},{"cell_type":"code","source":"def evaluator_node(state: dict) -> dict:\n    question_en = state[\"translated_input\"]\n    context_str = \"\\n\".join(\n        f\"Q: {r['question']}\\nA: {r['answer']}\" for r in state[\"retrieved_chunks\"]\n    )\n\n    formatted_prompt = QA_PROMPT.format(\n        prompt=f\"Context:\\n{context_str}\\n\\nQuestion:\\n{question_en}\",\n        response=state[\"ai_response_en\"]\n    )\n\n    verbose_eval_response = claude_client.messages.create(\n        model=\"claude-3-5-haiku-20241022\",\n        max_tokens=1024,\n        temperature=0.0,\n        messages=[\n            {\"role\": \"user\", \"content\": formatted_prompt}\n        ]\n    )\n    verbose_eval = verbose_eval_response.content[0].text\n\n    score_response = claude_client.messages.create(\n        model=\"claude-3-5-haiku-20241022\",\n        max_tokens=10,\n        temperature=0.0,\n        messages=[\n            {\"role\": \"user\", \"content\": formatted_prompt},\n            {\"role\": \"assistant\", \"content\": verbose_eval},\n            {\"role\": \"user\", \"content\": \"Give a rating between 1 and 5, only the number.\"}\n        ]\n    )\n    score_text = score_response.content[0].text.strip()\n\n    match = re.search(r\"[1-5]\", score_text)\n    rating_value = int(match.group()) if match else 3\n\n    state[\"ai_rating_text\"] = verbose_eval\n    state[\"ai_rating_score\"] = str(rating_value)\n    state[\"ai_rating_confidence\"] = rating_value / 5.0\n\n    return state","metadata":{"execution":{"iopub.status.busy":"2025-04-21T06:45:20.360851Z","iopub.execute_input":"2025-04-21T06:45:20.361109Z","iopub.status.idle":"2025-04-21T06:45:20.373586Z","shell.execute_reply.started":"2025-04-21T06:45:20.361086Z","shell.execute_reply":"2025-04-21T06:45:20.372676Z"},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"## 9. Back-Translation: Converting Answers to Kurdish\n\nOnce we have a high-quality English answer, we need to translate it back to Sorani Kurdish in a way that preserves medical accuracy.\n\nThe controlled translation approach:\n1. Uses a specialized prompt that emphasizes preserving medical details\n2. Instructs the model to keep medical terms in English if no clear Kurdish equivalent exists\n3. Prevents summarization to ensure all medical information is retained\n4. Handles translation errors gracefully with fallbacks\n\nThis specialized translation is critical for ensuring medical information remains accurate across languages.\n","metadata":{}},{"cell_type":"markdown","source":"# 9.1 Controlled Translation Node","metadata":{}},{"cell_type":"code","source":"def controlled_translate_node(state: dict) -> dict:\n    \"\"\"\n    Translates the AI-generated English answer into Sorani Kurdish (CKB),\n    preserving medical accuracy and detail, using Gemini.\n    \"\"\"\n    text = state[\"ai_response_en\"]\n\n    prompt = f\"\"\"\n    Translate the following English medical answer to **Sorani Kurdish**, keeping the same level of detail.\n\n    Do **not summarize**. Translate all sentences unless medically unsafe.\n\n    If any English medical term has no clear Kurdish equivalent, keep it in Latin/English script.\n\n    Answer to translate:\n    {text}\n    \"\"\"\n\n    config = types.GenerateContentConfig(temperature=0.3)\n\n    try:\n        response = client.models.generate_content(\n            model=\"gemini-1.5-pro-latest\",\n            config=config,\n            contents=[prompt]\n        )\n        translated = response.text.strip()\n    except Exception as e:\n        print(f\"Gemini translation error: {e}\")\n        translated = \"Translation failed. Please try again.\"\n\n    state[\"ai_response_ckb\"] = translated\n    return state\n","metadata":{"execution":{"iopub.status.busy":"2025-04-21T06:45:20.374687Z","iopub.execute_input":"2025-04-21T06:45:20.374955Z","iopub.status.idle":"2025-04-21T06:45:20.391287Z","shell.execute_reply.started":"2025-04-21T06:45:20.374931Z","shell.execute_reply":"2025-04-21T06:45:20.390416Z"},"trusted":true},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"### 9.2 Medical Terminology Extraction\n\nTo enhance transparency and understanding, we extract key medical terms from the English answer:\n\n1. The function identifies medical terminology using Gemini\n2. Returns a structured list of medical terms in lowercase\n3. Provides these terms as part of the response\n4. Helps users understand specialized terminology\n\nThis feature helps bridge the knowledge gap by highlighting important medical concepts that might require further explanation.\n","metadata":{}},{"cell_type":"markdown","source":"# 9.2 Medical Term Extraction Node","metadata":{}},{"cell_type":"code","source":"def extract_terms_node(state: dict) -> dict:\n    \"\"\"\n    Extracts medical terms from the English answer using Gemini,\n    and stores them in the state as a list of lowercase strings.\n    \"\"\"\n    import ast\n\n    answer_text = state.get(\"ai_response_en\", \"\")\n\n    prompt = \"\"\"\nExtract the **medical terms** from the following medical answer.\nReturn them as a **Python list** of lowercase strings in strict syntax (e.g., [\"diabetes\", \"insulin\"]).\nOnly return the list ‚Äî no explanation.\n\nAnswer:\n\"\"\" + answer_text\n\n    config = types.GenerateContentConfig(temperature=0.3)\n\n    try:\n        response = client.models.generate_content(\n            model=\"gemini-1.5-pro-latest\",\n            config=config,\n            contents=[prompt]\n        )\n\n        raw = response.text.strip()\n\n        # === Try direct parsing\n        if raw.startswith(\"[\") and raw.endswith(\"]\"):\n            parsed = ast.literal_eval(raw)\n            if isinstance(parsed, list):\n                state[\"medical_terms\"] = parsed\n                return state\n\n        # === Fallback: parse assignment-style lines\n        for line in raw.splitlines():\n            if \"medical_terms\" in line and \"=\" in line:\n                _, list_str = line.split(\"=\", 1)\n                parsed = ast.literal_eval(list_str.strip())\n                if isinstance(parsed, list):\n                    state[\"medical_terms\"] = parsed\n                    return state\n\n        # === If nothing works\n        raise ValueError(\"Output could not be parsed as a list\")\n\n    except Exception as e:\n        print(f\"Term extraction failed: {e}\")\n        state[\"medical_terms\"] = []\n\n    return state","metadata":{"execution":{"iopub.status.busy":"2025-04-21T06:45:20.392378Z","iopub.execute_input":"2025-04-21T06:45:20.392656Z","iopub.status.idle":"2025-04-21T06:45:20.407490Z","shell.execute_reply.started":"2025-04-21T06:45:20.392632Z","shell.execute_reply":"2025-04-21T06:45:20.406633Z"},"trusted":true},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"## 10. Fallback Mechanism: Search Grounding\n\nIf our primary RAG system can't find relevant information, we implement a fallback using Gemini's built-in search capability:\n\n1. The function sends the question to Gemini 2.0 with search tools enabled\n2. Gemini grounds its response in up-to-date web information\n3. The system marks the response as coming from web search\n4. This provides a safety net for questions outside our medical database\n\nThis ensures we can still provide useful information even when our primary knowledge base lacks relevant content.\n","metadata":{}},{"cell_type":"code","source":"def search_grounding_node(state: dict) -> dict:\n    query = state[\"translated_input\"]\n\n    print(\"üîé NojdarBot is grounding via Gemini‚Äôs built-in search tool...\")\n\n    config_with_search = types.GenerateContentConfig(\n        tools=[types.Tool(google_search=types.GoogleSearch())],\n        temperature=0.3\n    )\n\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=[f\"\"\"Search online and answer this medical question **with links or source names** if possible:{query}\"\"\"],        \n        config=config_with_search\n    )\n\n    answer_text = response.candidates[0].content.parts[0].text\n    state[\"ai_response_en\"] = answer_text\n    state[\"grounded_from_search\"] = True\n    return state","metadata":{"execution":{"iopub.status.busy":"2025-04-21T06:45:20.408454Z","iopub.execute_input":"2025-04-21T06:45:20.408727Z","iopub.status.idle":"2025-04-21T06:45:20.426651Z","shell.execute_reply.started":"2025-04-21T06:45:20.408704Z","shell.execute_reply":"2025-04-21T06:45:20.425734Z"},"trusted":true},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"## 11. State Management: Defining System Data Flow\n\nIn this section, we define the state object that flows through our pipeline. Each field represents a specific piece of data that is:\n1. Generated by one node\n2. Consumed by subsequent nodes\n3. Eventually used to build the final response\n\nThe `BotState` class uses Python's `TypedDict` to define a structured state object with:\n- `user_input`: Original Kurdish question from the user\n- `translated_input`: English translation of the question\n- `retrieved_chunks`: Medical information passages retrieved from our knowledge base\n- `retrieved_metadata`: Source information for the retrieved chunks\n- `ai_response_en`: Generated answer in English\n- `ai_response_ckb`: Translated answer in Kurdish\n- `ai_rating_score`: Quality evaluation score (1-5)\n- `ai_rating_confidence`: Confidence score for the evaluation (0.0-1.0)\n- `ai_rating_text`: Detailed feedback on answer quality\n- `medical_terms`: Extracted medical terminology with definitions\n- `fallback`: Flag indicating if the system should use fallback mechanisms\n\nThis strongly-typed approach ensures consistent data flow and helps identify potential issues during development.\n","metadata":{}},{"cell_type":"code","source":"from typing import TypedDict, Optional, List\nfrom langgraph.graph import StateGraph, END\nclass BotState(TypedDict):\n    user_input: str\n    translated_input: Optional[str]\n    retrieved_chunks: Optional[List[dict]]\n    retrieved_metadata: Optional[List[dict]]\n    ai_response_en: Optional[str]\n    ai_response_ckb: Optional[str]\n    ai_rating_score: Optional[str]\n    ai_rating_confidence: Optional[float]\n    ai_rating_text: Optional[str]\n    medical_terms: Optional[List[str]]\n    fallback: Optional[bool]","metadata":{"execution":{"iopub.status.busy":"2025-04-21T06:45:20.427822Z","iopub.execute_input":"2025-04-21T06:45:20.428171Z","iopub.status.idle":"2025-04-21T06:45:21.111868Z","shell.execute_reply.started":"2025-04-21T06:45:20.428138Z","shell.execute_reply":"2025-04-21T06:45:21.111119Z"},"trusted":true},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"## 12. System Integration: Building the Complete Pipeline\n\nWith all components defined, we now build the complete processing pipeline using LangGraph:\n\n1. Define the state structure that will flow through our pipeline\n2. Create a graph connecting all processing nodes in the correct order\n3. Set up conditional paths for quality-based fallback mechanisms\n4. Compile the graph into an executable pipeline\n\nThe graph workflow proceeds as follows:\n1. **Intake**: Translates user input from Kurdish to English\n2. **Retriever**: Finds relevant medical information in the knowledge base\n3. **Recommender**: Generates a medical answer using retrieved information\n4. **Evaluator**: Assesses answer quality and determines if fallback is needed\n5. **Conditional Path**:\n   - If quality is sufficient ‚Üí proceed to translation\n   - If quality is insufficient ‚Üí trigger search-based fallback mechanism\n6. **Translator**: Converts English answer back to Kurdish\n7. **Term Extractor**: Identifies and explains key medical terminology\n\nThis structured approach makes the system modular, extensible, and easy to debug when issues arise. The conditional branching ensures quality control, triggering fallback mechanisms when needed to maintain answer reliability.\n","metadata":{}},{"cell_type":"code","source":"graph = StateGraph(BotState)\n\n# Add all your nodes\ngraph.add_node(\"intake\", intake_node)\ngraph.add_node(\"retriever\", retriever_node)\ngraph.add_node(\"recommender\", recommender_node)\ngraph.add_node(\"evaluator\", evaluator_node)\ngraph.add_node(\"search_grounding\", search_grounding_node)  # ‚Üê Added\ngraph.add_node(\"translator\", controlled_translate_node)\ngraph.add_node(\"term_extractor\", extract_terms_node)\n\n# Set entry point\ngraph.set_entry_point(\"intake\")\n\n# Core flow\ngraph.add_edge(\"intake\", \"retriever\")\ngraph.add_edge(\"retriever\", \"recommender\")\ngraph.add_edge(\"recommender\", \"evaluator\")\n\n# Conditional path: fallback ‚Üí search_grounding ‚Üí translator\ngraph.add_conditional_edges(\n    \"evaluator\",\n    lambda state: \"search_grounding\" if state.get(\"fallback\", False) else \"translator\",\n    {\n        \"search_grounding\": \"search_grounding\",\n        \"translator\": \"translator\"\n    }\n)\n\n# Ensure translator runs even after fallback\ngraph.add_edge(\"search_grounding\", \"translator\")\n\n# Final steps\ngraph.add_edge(\"translator\", \"term_extractor\")\ngraph.add_edge(\"term_extractor\", END)\n\n# Compile the graph\nnojdarbot_graph = graph.compile()","metadata":{"execution":{"iopub.status.busy":"2025-04-21T06:45:21.112741Z","iopub.execute_input":"2025-04-21T06:45:21.113001Z","iopub.status.idle":"2025-04-21T06:45:21.136412Z","shell.execute_reply.started":"2025-04-21T06:45:21.112978Z","shell.execute_reply":"2025-04-21T06:45:21.135317Z"},"trusted":true},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"## 13. Pipeline Invocation: Stateless Function Interface\n\nThis function provides a clean, stateless interface to the NojdarBot pipeline. It:\n1. Takes a single Kurdish medical question as input\n2. Initializes the state object with this input\n3. Invokes the full processing pipeline\n4. Extracts and formats the final results into a structured response\n\nThe returned dictionary contains:\n- The Kurdish answer summary\n- The original English answer (for reference)\n- Urgency assessment (currently fixed as \"low\")\n- Extracted medical terms with definitions\n- Source information for transparency\n- Context passages used to generate the answer\n- Evaluation metrics including quality rating and confidence\n\nThis function makes it easy to integrate NojdarBot into various applications without needing to understand the internal state management details.\n","metadata":{}},{"cell_type":"code","source":"def nojdarbot_pipeline(user_input_ckb: str) -> dict:\n    input_state = {\n        \"user_input\": user_input_ckb\n    }\n    final_state = nojdarbot_graph.invoke(input_state)\n\n    return {\n        \"summary\": final_state.get(\"ai_response_ckb\"),\n        \"english_answer\": final_state.get(\"ai_response_en\"),\n        \"urgency\": \"low\",\n        \"medical_terms\": final_state.get(\"medical_terms\", []),\n        \"sources\": final_state.get(\"retrieved_chunks\", []),\n        \"context_used\": [f\"{r['question']}\\n{r['answer']}\" for r in final_state.get(\"retrieved_chunks\", [])],\n        \"evaluation\": {\n            \"rating_label\": final_state.get(\"ai_rating_score\"),\n            \"rating_value\": int(final_state.get(\"ai_rating_confidence\", 0.0) * 5),\n            \"details\": final_state.get(\"ai_rating_text\")\n    }}","metadata":{"execution":{"iopub.status.busy":"2025-04-21T06:45:21.137435Z","iopub.execute_input":"2025-04-21T06:45:21.137748Z","iopub.status.idle":"2025-04-21T06:45:21.148811Z","shell.execute_reply.started":"2025-04-21T06:45:21.137722Z","shell.execute_reply":"2025-04-21T06:45:21.147866Z"},"trusted":true},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"## 14. User Interaction: Human-in-the-Loop Node\n\nThis function implements the human interaction component of our system, allowing for:\n1. Displaying the system's response to the user\n2. Collecting the next user input\n3. Handling exit commands in multiple languages\n4. Updating the state with new user input\n\nThis follows the standard LangGraph pattern for human-in-the-loop nodes, making it easy to integrate with the rest of the pipeline. The function is designed to work in interactive environments like Jupyter notebooks and command-line interfaces.\n","metadata":{}},{"cell_type":"code","source":"def human_node(state: BotState) -> BotState:\n    \"\"\"Display the last model message to the user, and receive the user's input.\n    This follows the same pattern as the LangGraph notebook example.\"\"\"\n    \n    # If there's a response to display, show it first\n    if \"ai_response_ckb\" in state and state[\"ai_response_ckb\"]:\n        print(\"\\nNojdarBot:\", state[\"ai_response_ckb\"])\n    \n    # Get user input\n    user_input = input(\"\\nüßë‚Äç‚öïÔ∏è You: \")\n    \n    # Check for exit commands\n    if user_input.lower() in {\"q\", \"quit\", \"exit\", \"goodbye\", \"ÿØ€ïÿ±⁄ÜŸàŸàŸÜ\"}:\n        return state | {\"finished\": True, \"user_input\": user_input}\n    \n    # Return the state with the new user input\n    return state | {\"user_input\": user_input}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T06:45:21.149944Z","iopub.execute_input":"2025-04-21T06:45:21.150229Z","iopub.status.idle":"2025-04-21T06:45:21.163900Z","shell.execute_reply.started":"2025-04-21T06:45:21.150204Z","shell.execute_reply":"2025-04-21T06:45:21.163054Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"## 15. User Interface: Displaying Results\n\nThis function handles the presentation layer of NojdarBot, providing two display modes:\n\n1. **Detailed Mode**: Comprehensive view with:\n   - English and Kurdish answers\n   - Confidence indicators with color coding\n   - Medical terminology explanations\n   - Source attribution\n   - Session tracking information\n   - Fallback warnings when appropriate\n\n2. **Minimal Mode**: Streamlined view with:\n   - Kurdish answer prominently displayed\n   - Collapsible English translation\n   - Basic confidence indicator\n   - Session ID for tracking\n\nThe function supports right-to-left text rendering for Kurdish and uses Markdown formatting for clear visual hierarchy. The confidence badges are color-coded based on the system's confidence level (green for high, orange for medium, red for low confidence), providing users with an immediate visual cue about answer reliability.\n","metadata":{}},{"cell_type":"code","source":"from IPython.display import Markdown, display\nimport uuid\nfrom datetime import datetime\n\ndef display_nojdarbot(response: dict, mode: str = \"detailed\"):\n    \"\"\"\n    Displays NojdarBot's response in a user-friendly format, with options for\n    detailed or minimal output. Includes timestamp, session ID, confidence\n    indicator, and handling for fallback responses.\n    \"\"\"\n    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    session_id = str(uuid.uuid4())[:8]\n    eval_data = response.get(\"evaluation\", {})\n    confidence = eval_data.get(\"rating_value\", 0) / 5.0\n\n    # Determine confidence badge color\n    if confidence >= 0.8:\n        color = \"#4CAF50\"  # Green\n    elif confidence >= 0.6:\n        color = \"#FF9800\"  # Orange\n    else:\n        color = \"#F44336\"  # Red\n\n    if mode == \"minimal\":\n        # Minimal Display\n        display(Markdown(f\"\"\"\n### ü©∫ **NojdarBot Response**\n\n<div dir=\"rtl\" style=\"text-align: right; font-family: 'Noto Naskh Arabic', 'Segoe UI', sans-serif; font-size: 16px; background-color: #eef9f0; padding: 12px; border-radius: 8px; border-left: 4px solid #4CAF50;\">\n{response.get(\"summary\", \"ŸÜ€ïÿØ€Üÿ≤ÿ±ÿß€å€ïŸà€ï.\")}\n</div>\n\n<details>\n<summary><strong>English Version</strong></summary>\n<blockquote>{response.get(\"english_answer\", \"N/A\")}</blockquote>\n</details>\n\n<small><span style=\"color: white; background-color: {color}; padding: 4px 8px; border-radius: 5px;\">\nConfidence: {eval_data.get(\"rating_value\", \"?\")}/5 ({eval_data.get(\"rating_label\", \"N/A\")})\n</span></small>\n\n<p style=\"color: grey; font-size: 12px;\">Session ID: {session_id} | {timestamp}</p>\n\"\"\"))\n        return\n\n    # Detailed Display\n    separator = \"\\n\\n\"\n    summary = response.get(\"summary\") or \"\"\n\n    # Fallback warning\n    if summary.startswith(\"ÿ®ÿ®Ÿàÿ±€ï\"):\n        display(Markdown(f\"\"\"\n> ‚ö†Ô∏è **No confident answer found**. NojdarBot returned a fallback response.\n> Try rephrasing your question or ask a simpler one.\n\"\"\"))\n\n    # Main Answers\n    display(Markdown(\"### ü©∫ **NojdarBot Medical Assistant**\"))\n\n    # Web Search Grounding Notice\n    if response.get(\"grounded_from_search\"):\n        display(Markdown(\"> üåê This answer was generated using real-time web search.\"))\n\n    # English Answer block\n    display(Markdown(f\"\"\"\n#### **English Answer**\n> {response.get('english_answer', 'N/A')}\n\"\"\"))\n\n    # Kurdish Answer\n    display(Markdown(f\"\"\"\n#### **Kurdish Answer**\n<div dir=\"rtl\" style=\"text-align: right; font-family: 'Noto Naskh Arabic', 'Segoe UI', sans-serif; font-size: 16px; background-color: #f9f9f9; padding: 10px; border-radius: 6px;\">\n{response.get('summary', 'N/A')}\n</div>\n\n<small><span style=\"color: white; background-color: {color}; padding: 4px 8px; border-radius: 5px;\">\nConfidence: {eval_data.get(\"rating_value\", \"?\")}/5 ({eval_data.get(\"rating_label\", \"N/A\")})\n</span></small>\n\"\"\"))\n\n    # Context\n    display(Markdown(f\"\"\"\n#### **Context Used**\n<pre style=\"background-color: #f7f7f7; padding: 10px; border-radius: 5px; white-space: pre-wrap;\">\n{separator.join(response.get(\"context_used\", []))}\n</pre>\n\n#### **Detected Medical Terms**\n<code>{', '.join(response.get('medical_terms', []))}</code>\n\"\"\"))\n\n    # Evaluation Details\n    display(Markdown(f\"\"\"\n#### **Evaluation Explanation**\n<details>\n<summary><strong>Show full evaluation (click to expand)</strong></summary>\n<div style=\"padding: 10px; background-color: #f9f9f9; margin-top: 5px;\">\n{eval_data.get('details', 'N/A')}\n</div>\n</details>\n\"\"\"))\n\n    # Sources\n    display(Markdown(\"#### **Context Sources**\"))\n    for i, item in enumerate(response.get(\"sources\", []), 1):\n        display(Markdown(f\"\"\"\n<details>\n<summary><strong>{i}. Question:</strong> {item.get('question', 'N/A')}</summary>\n<div style=\"padding: 10px; background-color: #f9f9f9; margin-top: 5px;\">\n<strong>Answer:</strong> {item.get('answer', 'N/A')}\n</div>\n</details>\n\"\"\"))\n\n    # Footer\n    display(Markdown(f\"\"\"\n<p style=\"color: grey; font-size: 12px;\">Session ID: {session_id} | Generated on: {timestamp}</p>\n\"\"\"))","metadata":{"execution":{"iopub.status.busy":"2025-04-21T06:45:21.165103Z","iopub.execute_input":"2025-04-21T06:45:21.165456Z","iopub.status.idle":"2025-04-21T06:45:21.183293Z","shell.execute_reply.started":"2025-04-21T06:45:21.165420Z","shell.execute_reply":"2025-04-21T06:45:21.182326Z"},"trusted":true},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"### 15.2 Kurdish Answer Display Section\n\nThe Kurdish answer block is displayed with specific considerations for right-to-left text:\n\n1. **RTL Text Direction**: Uses `dir=\"rtl\"` HTML attribute to ensure proper text flow\n2. **Right Alignment**: Text aligned right for natural Kurdish reading experience\n3. **Font Selection**: Prioritizes 'Noto Naskh Arabic' for best Kurdish character rendering\n4. **Visual Styling**:\n   - Light green background (`#eef9f0`) for better readability\n   - 12px padding for comfortable text spacing\n   - Rounded corners (8px border-radius) for modern UI appearance\n   - Left border accent (4px solid green) for visual hierarchy\n5. **Fallback Detection**: Special handling for fallback answers that start with \"ÿ®ÿ®Ÿàÿ±€ï\"\n\nThis section uses HTML within Markdown to achieve the necessary styling and text direction\ncontrol, which is essential for Kurdish (Sorani) text display in Jupyter environments.\n","metadata":{}},{"cell_type":"code","source":"FEEDBACK_LOG_PATH = \"logs/feedback_log.jsonl\"\n\nif os.path.exists(FEEDBACK_LOG_PATH):\n    with open(FEEDBACK_LOG_PATH, \"r\", encoding=\"utf-8\") as f:\n        feedback_log = [json.loads(line) for line in f.readlines()]\nelse:\n    feedback_log = []","metadata":{"execution":{"iopub.status.busy":"2025-04-21T06:45:21.184185Z","iopub.execute_input":"2025-04-21T06:45:21.184503Z","iopub.status.idle":"2025-04-21T06:45:21.200994Z","shell.execute_reply.started":"2025-04-21T06:45:21.184478Z","shell.execute_reply":"2025-04-21T06:45:21.200109Z"},"trusted":true},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"log function","metadata":{}},{"cell_type":"code","source":"def log_feedback(entry: dict, path: str = \"logs/feedback_log.jsonl\"):\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    with open(path, \"a\", encoding=\"utf-8\") as f:\n        f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n\n\ndef collect_user_feedback(question, answer, context_list, structured_eval, confidence, verbose_eval, user_id=\"anon\"):\n    print(\"ÿ¶€ïŸÖ Ÿà€ï⁄µÿßŸÖ€ïÿ™ Ÿæ€é ⁄Ü€ÜŸÜ ÿ®ŸàŸàÿü\")\n    print(\"[1] ÿ≤€Üÿ± ÿÆÿ±ÿßŸæ   [2] ÿÆÿ±ÿßŸæ   [3] ÿÆÿ±ÿßŸæ ŸÜ€ïÿ®ŸàŸà   [4] ÿ®ÿßÿ¥   [5] ÿ≤€Üÿ± ÿ®ÿßÿ¥\")\n\n    try:\n        choice = int(input(\"Ÿá€ï⁄µÿ®⁄òÿßÿ±ÿØŸÜ (1‚Äì5): \").strip())\n        if choice not in range(1, 6):\n            raise ValueError\n    except:\n        print(\"‚ö†Ô∏è Ÿá€ï⁄µ€ï€å€ï⁄© ⁄ïŸàŸà€åÿØÿß.'Ÿ£' Ÿá€ï⁄µÿ®⁄ò€éÿ±ÿØÿ±ÿß.\")\n        choice = 3  # Default to neutral\n\n\n    timestamp = datetime.now().isoformat()\n    entry = {\n        \"timestamp\": timestamp,\n        \"user_id\": user_id,\n        \"question\": question,\n        \"answer\": answer,\n        \"context\": context_list,\n        \"gemini_rating\": structured_eval.name,\n        \"confidence\": confidence,\n        \"reasoning\": verbose_eval,\n        \"user_rating_num\": choice,\n        \"agent_version\": \"v1.0\",  # in case your prompt changes later\n        \"mode\": \"notebook\"\n    }\n\n    \n    feedback_log.append(entry)\n    log_feedback(entry)  # Save immediately to disk too\n    print(\"‚úÖ Feedback saved!\")","metadata":{"execution":{"iopub.status.busy":"2025-04-21T06:45:21.201941Z","iopub.execute_input":"2025-04-21T06:45:21.202253Z","iopub.status.idle":"2025-04-21T06:45:21.216648Z","shell.execute_reply.started":"2025-04-21T06:45:21.202202Z","shell.execute_reply":"2025-04-21T06:45:21.215695Z"},"trusted":true},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"##  Test Q","metadata":{}},{"cell_type":"code","source":"def interactive_nojdarbot_session():\n    from IPython.display import display\n    import uuid\n\n    print(\"ü©∫ Ÿæÿ±ÿ≥€åÿßÿ±€å ÿ™€ïŸÜÿØÿ±Ÿàÿ≥ÿ™€å ÿÆ€Üÿ™ ÿ®€ï ⁄©Ÿàÿ±ÿØ€å (ÿ≥€Üÿ±ÿßŸÜ€å) ÿ®ŸÜŸàŸàÿ≥€ï.\")\n    print(\"ÿ®ŸÜŸàŸàÿ≥Ÿá 'q' €åÿßŸÜ 'quit' ÿ®€Ü ÿ™€ïŸàÿßŸà⁄©ÿ±ÿØŸÜ€å Ÿàÿ™ŸàŸà€é⁄ò.\\n\")\n\n    last_response = None\n    last_question = None\n\n    while True:\n        try:\n            user_input = input(\"üßë‚Äç‚öïÔ∏è ÿ™€Ü: \").strip()\n        except (KeyboardInterrupt, EOFError):\n            print(\"\\nSession interrupted. Proceeding to feedback.\")\n            break\n\n        if user_input.lower() in {\"q\", \"quit\", \"exit\"}:\n            print(\"\\nGot it ‚Äî let's collect feedback on the last answer.\")\n            break\n\n        if not user_input:\n            print(\"ÿ™⁄©ÿß€å€ï Ÿæÿ±ÿ≥€åÿßÿ±€é⁄© ÿ®ŸÜŸàŸàÿ≥€ï.\")\n            continue\n\n        display_mode = \"\"\n        while display_mode not in {\"detailed\", \"minimal\"}:\n            display_mode = input(\"Please choose either 'detailed' if you want English context also or 'minimal' for Kurdish only.\").strip().lower()\n            if display_mode not in {\"detailed\", \"minimal\"}:\n                print(\"Please choose either 'detailed' if you want English context also or 'minimal' for Kurdish only.\")\n\n        last_question = user_input\n        state = {\"user_input\": user_input,\n                \"chat_history\": []}\n        final_state = nojdarbot_graph.invoke(state)\n\n        last_response = {\n            \"summary\": final_state.get(\"ai_response_ckb\"),\n            \"english_answer\": final_state.get(\"ai_response_en\"),\n            \"urgency\": \"low\",\n            \"medical_terms\": final_state.get(\"medical_terms\", []),\n            \"sources\": final_state.get(\"retrieved_chunks\", []),\n            \"context_used\": [f\"{r['question']}\\n{r['answer']}\" for r in final_state.get(\"retrieved_chunks\", [])],\n            \"evaluation\": {\n                \"rating_label\": final_state.get(\"ai_rating_score\"),\n                \"rating_value\": int(final_state.get(\"ai_rating_confidence\", 0.0) * 5),\n                \"details\": final_state.get(\"ai_rating_text\")\n            },\n            \"ai_rating_confidence\": final_state.get(\"ai_rating_confidence\", 0.0),\n            \"ai_rating_text\": final_state.get(\"ai_rating_text\", \"\")\n        }\n\n        display_nojdarbot(last_response, mode=display_mode)\n\n    # === Collect feedback after loop ends ===\n    if last_response:\n        try:\n            score = last_response[\"evaluation\"][\"rating_value\"]\n            structured_enum = AnswerRating(str(score))\n        except Exception:\n            structured_enum = AnswerRating.OK\n\n        collect_user_feedback(\n            question=last_question,\n            answer=last_response[\"english_answer\"],\n            context_list=last_response[\"context_used\"],\n            structured_eval=structured_enum,\n            confidence=last_response[\"ai_rating_confidence\"],\n            verbose_eval=last_response[\"ai_rating_text\"]\n        )\n    else:\n        print(\"‚ö†Ô∏è No valid response was generated to collect feedback on.\")","metadata":{"execution":{"iopub.status.busy":"2025-04-21T06:45:21.217704Z","iopub.execute_input":"2025-04-21T06:45:21.218072Z","iopub.status.idle":"2025-04-21T06:45:21.232107Z","shell.execute_reply.started":"2025-04-21T06:45:21.218044Z","shell.execute_reply":"2025-04-21T06:45:21.231012Z"},"trusted":true},"outputs":[],"execution_count":33},{"cell_type":"code","source":"interactive_nojdarbot_session()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T06:45:21.233112Z","iopub.execute_input":"2025-04-21T06:45:21.233525Z","iopub.status.idle":"2025-04-21T06:50:06.079412Z","shell.execute_reply.started":"2025-04-21T06:45:21.233487Z","shell.execute_reply":"2025-04-21T06:50:06.078470Z"}},"outputs":[{"name":"stdout","text":"ü©∫ Ÿæÿ±ÿ≥€åÿßÿ±€å ÿ™€ïŸÜÿØÿ±Ÿàÿ≥ÿ™€å ÿÆ€Üÿ™ ÿ®€ï ⁄©Ÿàÿ±ÿØ€å (ÿ≥€Üÿ±ÿßŸÜ€å) ÿ®ŸÜŸàŸàÿ≥€ï.\nÿ®ŸÜŸàŸàÿ≥Ÿá 'q' €åÿßŸÜ 'quit' ÿ®€Ü ÿ™€ïŸàÿßŸà⁄©ÿ±ÿØŸÜ€å Ÿàÿ™ŸàŸà€é⁄ò.\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"üßë‚Äç‚öïÔ∏è ÿ™€Ü:  ŸÖÿßŸà€ï€å ÿØŸàŸà Ÿá€ïŸÅÿ™€ï€å€ï ⁄©€Ü⁄©€ï€å€ï⁄©€å Ÿàÿ¥⁄©ŸÖ Ÿá€ï€å€ï Ÿà ÿ™ÿßŸÖ Ÿá€ïÿ≥ÿ™ Ÿæ€éŸÜÿß⁄©€ïŸÖ. ÿØ€ïÿ®€éÿ™ ⁄Ü€å ÿ®⁄©€ïŸÖÿü\nPlease choose either 'detailed' if you want English context also or 'minimal' for Kurdish only. minimal\n"},{"name":"stdout","text":"\nüß† Chat History So Far:\n\nTurn 1:\nUser: ŸÖÿßŸà€ï€å ÿØŸàŸà Ÿá€ïŸÅÿ™€ï€å€ï ⁄©€Ü⁄©€ï€å€ï⁄©€å Ÿàÿ¥⁄©ŸÖ Ÿá€ï€å€ï Ÿà ÿ™ÿßŸÖ Ÿá€ïÿ≥ÿ™ Ÿæ€éŸÜÿß⁄©€ïŸÖ. ÿØ€ïÿ®€éÿ™ ⁄Ü€å ÿ®⁄©€ïŸÖÿü\nAgent: A dry cough and loss of taste can be related to several things, including respiratory infections. Since you've been experiencing these symptoms for two weeks, it's important to see a doctor to determine the cause and receive appropriate treatment.  They will be able to evaluate your specific situation and advise you on the best course of action.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n### ü©∫ **NojdarBot Response**\n\n<div dir=\"rtl\" style=\"text-align: right; font-family: 'Noto Naskh Arabic', 'Segoe UI', sans-serif; font-size: 16px; background-color: #eef9f0; padding: 12px; border-radius: 8px; border-left: 4px solid #4CAF50;\">\n⁄©€Ü⁄©€ï€å Ÿàÿ¥⁄© Ÿà ŸÑ€ïÿØ€ïÿ≥ÿ™ÿØÿßŸÜ€å ÿ™ÿßŸÖ (Loss of taste) ÿØ€ï⁄©ÿ±€éÿ™ Ÿæ€ï€åŸà€ïŸÜÿØ€å€åÿßŸÜ ÿ®€ï ⁄Ü€ïŸÜÿØ€åŸÜ ÿ¥ÿ™€ïŸà€ï Ÿá€ïÿ®€éÿ™ÿå ŸÑ€ïŸàÿßŸÜ€ï Ÿá€ïŸà⁄©ÿ±ÿØŸÜ€å ⁄©€Üÿ¶€ïŸÜÿØÿßŸÖ€å Ÿá€ïŸÜÿßÿ≥€ï. ⁄ÜŸàŸÜ⁄©€ï ÿ¶€ïŸÖ ŸÜ€åÿ¥ÿßŸÜÿßŸÜ€ïÿ™ ÿ®€Ü ŸÖÿßŸà€ï€å ÿØŸàŸà Ÿá€ïŸÅÿ™€ï€å€ï Ÿá€ï€å€ïÿå ⁄Øÿ±ŸÜ⁄Ø€ï ÿ≥€ïÿ±ÿØÿßŸÜ€å Ÿæÿ≤€åÿ¥⁄© ÿ®⁄©€ï€åÿ™ ÿ®€Ü ÿØ€åÿßÿ±€å⁄©ÿ±ÿØŸÜ€å Ÿá€Ü⁄©ÿßÿ±€ï⁄©€ï€å Ÿà Ÿà€ïÿ±⁄Øÿ±ÿ™ŸÜ€å ⁄Üÿßÿ±€ïÿ≥€ïÿ±€å ⁄ØŸàŸÜÿ¨ÿßŸà. ÿ¶€ïŸàÿßŸÜ ÿØ€ïÿ™ŸàÿßŸÜŸÜ ÿØ€ÜÿÆ€ï⁄©€ïÿ™ ÿ®€ï Ÿàÿ±ÿØ€å Ÿá€ï⁄µÿ®ÿ≥€ïŸÜ⁄Ø€éŸÜŸÜ Ÿà ÿ®ÿßÿ¥ÿ™ÿ±€åŸÜ ⁄ï€é⁄Ø€ï€å ⁄Üÿßÿ±€ïÿ≥€ïÿ±⁄©ÿ±ÿØŸÜÿ™ Ÿæ€é ÿ®⁄µ€éŸÜ.\n</div>\n\n<details>\n<summary><strong>English Version</strong></summary>\n<blockquote>A dry cough and loss of taste can be related to several things, including respiratory infections. Since you've been experiencing these symptoms for two weeks, it's important to see a doctor to determine the cause and receive appropriate treatment.  They will be able to evaluate your specific situation and advise you on the best course of action.</blockquote>\n</details>\n\n<small><span style=\"color: white; background-color: #4CAF50; padding: 4px 8px; border-radius: 5px;\">\nConfidence: 4/5 (4)\n</span></small>\n\n<p style=\"color: grey; font-size: 12px;\">Session ID: 394f3071 | 2025-04-21 06:47:50</p>\n"},"metadata":{}},{"output_type":"stream","name":"stdin","text":"üßë‚Äç‚öïÔ∏è ÿ™€Ü:  ÿØ€ïÿ™ŸàÿßŸÜ€å ÿØ€ïÿ±ŸÖÿßŸÜ€é⁄©ŸÖ Ÿæ€é ÿ®ÿØ€ï€åÿ™ÿü\nPlease choose either 'detailed' if you want English context also or 'minimal' for Kurdish only. minimal\n"},{"name":"stdout","text":"\nüß† Chat History So Far:\n\nTurn 1:\nUser: ÿØ€ïÿ™ŸàÿßŸÜ€å ÿØ€ïÿ±ŸÖÿßŸÜ€é⁄©ŸÖ Ÿæ€é ÿ®ÿØ€ï€åÿ™ÿü\nAgent: I cannot give you any medication. I am a medical assistant and am not authorized to prescribe or dispense medication.  You would need to speak with a doctor or other licensed prescriber.\nTerm extraction failed: Output could not be parsed as a list\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n### ü©∫ **NojdarBot Response**\n\n<div dir=\"rtl\" style=\"text-align: right; font-family: 'Noto Naskh Arabic', 'Segoe UI', sans-serif; font-size: 16px; background-color: #eef9f0; padding: 12px; border-radius: 8px; border-left: 4px solid #4CAF50;\">\nŸÖŸÜ ŸÜÿßÿ™ŸàÿßŸÜŸÖ Ÿá€å⁄Ü ÿØ€ïÿ±ŸÖÿßŸÜ€é⁄©ÿ™ ÿ®ÿØ€ïŸÖ. ŸÖŸÜ €åÿßÿ±€åÿØ€ïÿØ€ïÿ±€å Ÿæÿ≤€åÿ¥⁄©ŸÖ Ÿà ⁄ï€é⁄Ø€ïŸæ€éÿØÿ±ÿßŸà ŸÜ€åŸÖ ÿØ€ïÿ±ŸÖÿßŸÜ ÿ®ŸÜŸàŸàÿ≥ŸÖ €åÿßŸÜ ÿØÿßÿ®€ïÿ¥€å ÿ®⁄©€ïŸÖ.  Ÿæ€éŸà€åÿ≥ÿ™€ï ŸÇÿ≥€ï ŸÑ€ï⁄Ø€ï⁄µ Ÿæÿ≤€åÿ¥⁄©€é⁄© €åÿßŸÜ ⁄©€ïÿ≥€é⁄©€å ÿØ€å⁄©€ï€å ŸÖ€Ü⁄µ€ïÿ™Ÿæ€éÿØÿ±ÿßŸà ÿ®€Ü ŸÜŸàŸàÿ≥€åŸÜ€å ÿØ€ïÿ±ŸÖÿßŸÜ ÿ®⁄©€ï€åÿ™. (Min natwanm hech darman√™kt bdam. Mn yarƒ´daderƒ´ pizishkm u r√™ga pƒìdraw nƒìm darman bnwsm yan dabeshƒ´ bkam.  P√™wƒ´sta qsa lagha≈Ç pizƒ´shk√™k yan kes√™kƒ´ dƒ´kay m≈ç≈Çatpƒìdraw b≈ç nw≈´sƒ´nƒ´ darman bkayt.)\n</div>\n\n<details>\n<summary><strong>English Version</strong></summary>\n<blockquote>I cannot give you any medication. I am a medical assistant and am not authorized to prescribe or dispense medication.  You would need to speak with a doctor or other licensed prescriber.</blockquote>\n</details>\n\n<small><span style=\"color: white; background-color: #4CAF50; padding: 4px 8px; border-radius: 5px;\">\nConfidence: 5/5 (5)\n</span></small>\n\n<p style=\"color: grey; font-size: 12px;\">Session ID: 4e71077e | 2025-04-21 06:49:19</p>\n"},"metadata":{}},{"output_type":"stream","name":"stdin","text":"üßë‚Äç‚öïÔ∏è ÿ™€Ü:  q\n"},{"name":"stdout","text":"\nGot it ‚Äî let's collect feedback on the last answer.\nÿ¶€ïŸÖ Ÿà€ï⁄µÿßŸÖ€ïÿ™ Ÿæ€é ⁄Ü€ÜŸÜ ÿ®ŸàŸàÿü\n[1] ÿ≤€Üÿ± ÿÆÿ±ÿßŸæ   [2] ÿÆÿ±ÿßŸæ   [3] ÿÆÿ±ÿßŸæ ŸÜ€ïÿ®ŸàŸà   [4] ÿ®ÿßÿ¥   [5] ÿ≤€Üÿ± ÿ®ÿßÿ¥\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Ÿá€ï⁄µÿ®⁄òÿßÿ±ÿØŸÜ (1‚Äì5):  4\n"},{"name":"stdout","text":"‚úÖ Feedback saved!\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"import pandas as pd\n\npd.DataFrame(feedback_log)","metadata":{"execution":{"iopub.status.busy":"2025-04-21T06:50:06.082350Z","iopub.execute_input":"2025-04-21T06:50:06.082605Z","iopub.status.idle":"2025-04-21T06:50:06.723854Z","shell.execute_reply.started":"2025-04-21T06:50:06.082582Z","shell.execute_reply":"2025-04-21T06:50:06.722901Z"},"trusted":true},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"                    timestamp user_id                     question  \\\n0  2025-04-21T06:50:06.075659    anon  ÿØ€ïÿ™ŸàÿßŸÜ€å ÿØ€ïÿ±ŸÖÿßŸÜ€é⁄©ŸÖ Ÿæ€é ÿ®ÿØ€ï€åÿ™ÿü   \n\n                                              answer  \\\n0  I cannot give you any medication. I am a medic...   \n\n                                             context gemini_rating  \\\n0  [Do you have information about Medicines\\nSumm...     VERY_GOOD   \n\n   confidence                                          reasoning  \\\n0         1.0  Let me evaluate this response using the specif...   \n\n   user_rating_num agent_version      mode  \n0                4          v1.0  notebook  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>timestamp</th>\n      <th>user_id</th>\n      <th>question</th>\n      <th>answer</th>\n      <th>context</th>\n      <th>gemini_rating</th>\n      <th>confidence</th>\n      <th>reasoning</th>\n      <th>user_rating_num</th>\n      <th>agent_version</th>\n      <th>mode</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2025-04-21T06:50:06.075659</td>\n      <td>anon</td>\n      <td>ÿØ€ïÿ™ŸàÿßŸÜ€å ÿØ€ïÿ±ŸÖÿßŸÜ€é⁄©ŸÖ Ÿæ€é ÿ®ÿØ€ï€åÿ™ÿü</td>\n      <td>I cannot give you any medication. I am a medic...</td>\n      <td>[Do you have information about Medicines\\nSumm...</td>\n      <td>VERY_GOOD</td>\n      <td>1.0</td>\n      <td>Let me evaluate this response using the specif...</td>\n      <td>4</td>\n      <td>v1.0</td>\n      <td>notebook</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}